{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:05.830869Z",
     "start_time": "2024-08-19T15:45:04.819700Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mypy-protobuf 3.6.0 requires protobuf>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "feast 0.40.1 requires protobuf<5.0.0,>=4.24.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "codeflare-sdk 0.19.1 requires pydantic<2, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q onnx onnxruntime tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies for the model training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:08.983925Z",
     "start_time": "2024-08-19T15:45:05.835311Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 14:39:56.002438: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 14:39:56.002496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 14:39:56.022334: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 14:39:56.104497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 14:39:57.845502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tf2onnx\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might show TensorFlow messages, such as a \"Could not find TensorRT\" warning. You can ignore these messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV data\n",
    "\n",
    "The CSV data that you use to train the model contains the following fields:\n",
    "\n",
    "* **distancefromhome** - The distance from home where the transaction happened.\n",
    "* **distancefromlast_transaction** - The distance from the last transaction that happened.\n",
    "* **ratiotomedianpurchaseprice** - The ratio of purchased price compared to median purchase price.\n",
    "* **repeat_retailer** - If it's from a retailer that already has been purchased from before.\n",
    "* **used_chip** - If the credit card chip was used.\n",
    "* **usedpinnumber** - If the PIN number was used.\n",
    "* **online_order** - If it was an online order.\n",
    "* **fraud** - If the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Feast project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf2onnx 1.16.1 requires protobuf~=3.20, but you have protobuf 4.25.5 which is incompatible.\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "kfp-kubernetes 1.0.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "codeflare-sdk 0.19.1 requires pydantic<2, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Feast SDK Version: \"0.40.1\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q feast\n",
    "!feast version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forward Feastlogs to the notebook output\n",
    "import logging\n",
    "import sys\n",
    "from io import StringIO\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FEAST_REPO=feast_fraud/feature_repo\n",
      "env: ROOT_DIR=/opt/app-root/src/fraud-detection\n",
      "data  feature_store.yaml  __init__.py  __pycache__\n"
     ]
    }
   ],
   "source": [
    "%env FEAST_REPO=feast_fraud/feature_repo\n",
    "%env ROOT_DIR=/opt/app-root/src/fraud-detection\n",
    "!ls $FEAST_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/feast/feature_view.py:425: UserWarning: There are some mismatches in your feature view's registered entities. Please check if you have applied your entities correctly.Entities: ['customer'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "Deleted entity \u001b[1m\u001b[31mcustomer\u001b[0m\n",
      "Deleted feature view \u001b[1m\u001b[31mtraining\u001b[0m\n",
      "Deleted feature view \u001b[1m\u001b[31mtest\u001b[0m\n",
      "Deleted feature view \u001b[1m\u001b[31mvalidation\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94mNo changes to infrastructure\n",
      "NAME    DESCRIPTION    TYPE\n",
      "NAME    CLASS\n",
      "NAME    ENTITIES    TYPE\n"
     ]
    }
   ],
   "source": [
    "!feast -c $FEAST_REPO apply\n",
    "!feast -c $FEAST_REPO entities list\n",
    "!feast -c $FEAST_REPO data-sources list\n",
    "!feast -c $FEAST_REPO feature-views list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from feast import (\n",
    "    Entity,\n",
    "    FeatureService,\n",
    "    FeatureView,\n",
    "    Field,\n",
    "    FileSource,\n",
    "    PushSource,\n",
    "    RequestSource,\n",
    ")\n",
    "from feast.feature_store import FeatureStore\n",
    "from feast.feature_logging import LoggingConfig\n",
    "from feast.infra.offline_stores.file_source import FileLoggingDestination\n",
    "from feast.on_demand_feature_view import on_demand_feature_view\n",
    "from feast.types import Float32, Float64, Int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 14:40:30,704 - Registry cache expired, so refreshing\n",
      "2024-09-25 14:40:30,707 - Registry cache expired, so refreshing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the store\n",
    "store = FeatureStore(os.environ['FEAST_REPO'])\n",
    "print(store.list_entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "1. Add entity key to the tables\n",
    "1. Add event_ts field\n",
    "1. Convert to parquet files\n",
    "\n",
    "Since the original dataset was not considering the `customer` concept, we'll replicate the same setup: add a new field customer_id and apply the same value to each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_customer_id(df):\n",
    "    df['customer_id'] = 1\n",
    "\n",
    "def add_timestamps(df):\n",
    "    # Create time series: one entry every 1H, up to now\n",
    "    timestamps = pd.date_range(\n",
    "        end=pd.Timestamp.now().replace(microsecond=0), \n",
    "        periods=len(df), \n",
    "        freq='1H').to_frame(name=\"ts\", index=False)\n",
    "\n",
    "    timestamps['created'] = timestamps['ts']\n",
    "    df = pd.concat(objs=[df, timestamps], axis=1)\n",
    "    columns = df.columns.tolist()\n",
    "    columns.insert(0, columns.pop(8))\n",
    "    columns.insert(1, columns.pop(10))\n",
    "    columns.insert(2, columns.pop(10))\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4596/1258192443.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n",
      "/tmp/ipykernel_4596/1258192443.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n",
      "/tmp/ipykernel_4596/1258192443.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----xtrain-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   customer_id                     600000 non-null  int64         \n",
      " 1   created                         600000 non-null  datetime64[ns]\n",
      " 2   ts                              600000 non-null  datetime64[ns]\n",
      " 3   distance_from_home              600000 non-null  float64       \n",
      " 4   distance_from_last_transaction  600000 non-null  float64       \n",
      " 5   ratio_to_median_purchase_price  600000 non-null  float64       \n",
      " 6   repeat_retailer                 600000 non-null  float64       \n",
      " 7   used_chip                       600000 non-null  float64       \n",
      " 8   used_pin_number                 600000 non-null  float64       \n",
      " 9   online_order                    600000 non-null  float64       \n",
      " 10  fraud                           600000 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(8), int64(1)\n",
      "memory usage: 50.4 MB\n",
      "-----xval-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   customer_id                     200000 non-null  int64         \n",
      " 1   created                         200000 non-null  datetime64[ns]\n",
      " 2   ts                              200000 non-null  datetime64[ns]\n",
      " 3   distance_from_home              200000 non-null  float64       \n",
      " 4   distance_from_last_transaction  200000 non-null  float64       \n",
      " 5   ratio_to_median_purchase_price  200000 non-null  float64       \n",
      " 6   repeat_retailer                 200000 non-null  float64       \n",
      " 7   used_chip                       200000 non-null  float64       \n",
      " 8   used_pin_number                 200000 non-null  float64       \n",
      " 9   online_order                    200000 non-null  float64       \n",
      " 10  fraud                           200000 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(8), int64(1)\n",
      "memory usage: 16.8 MB\n",
      "-----xtest-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   customer_id                     200000 non-null  int64         \n",
      " 1   created                         200000 non-null  datetime64[ns]\n",
      " 2   ts                              200000 non-null  datetime64[ns]\n",
      " 3   distance_from_home              200000 non-null  float64       \n",
      " 4   distance_from_last_transaction  200000 non-null  float64       \n",
      " 5   ratio_to_median_purchase_price  200000 non-null  float64       \n",
      " 6   repeat_retailer                 200000 non-null  float64       \n",
      " 7   used_chip                       200000 non-null  float64       \n",
      " 8   used_pin_number                 200000 non-null  float64       \n",
      " 9   online_order                    200000 non-null  float64       \n",
      " 10  fraud                           200000 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(8), int64(1)\n",
      "memory usage: 16.8 MB\n"
     ]
    }
   ],
   "source": [
    "xtrain = pd.read_csv('data/train.csv')\n",
    "xval = pd.read_csv('data/validate.csv')\n",
    "xtest = pd.read_csv('data/test.csv')\n",
    "\n",
    "add_customer_id(xtrain)\n",
    "add_customer_id(xval)\n",
    "add_customer_id(xtest)\n",
    "\n",
    "xtrain = add_timestamps(xtrain)\n",
    "xval = add_timestamps(xval)\n",
    "xtest = add_timestamps(xtest)\n",
    "\n",
    "!rm data/*.parquet\n",
    "xtrain.to_parquet('data/train.parquet')\n",
    "xval.to_parquet('data/validate.parquet')\n",
    "xtest.to_parquet('data/test.parquet')\n",
    "\n",
    "print(\"-----xtrain-----\")\n",
    "xtrain.info()\n",
    "print(\"-----xval-----\")\n",
    "xval.info()\n",
    "print(\"-----xtest-----\")\n",
    "xtest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 1007410000 1007410000 8.8M Sep 25 14:40 data/test.parquet\n",
      "-rw-r--r--. 1 1007410000 1007410000  24M Sep 25 14:40 data/train.parquet\n",
      "-rw-r--r--. 1 1007410000 1007410000 8.8M Sep 25 14:40 data/validate.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/*parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Feature Store\n",
    "* Map parquet files to `FileSource`s\n",
    "* Define FeatureView for training purposes\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             CLASS\n",
      "train_source     <class 'feast.infra.offline_stores.file_source.FileSource'>\n",
      "validate_source  <class 'feast.infra.offline_stores.file_source.FileSource'>\n",
      "test_source      <class 'feast.infra.offline_stores.file_source.FileSource'>\n"
     ]
    }
   ],
   "source": [
    "# Create the FileSources\n",
    "train_source = FileSource(\n",
    "    name=\"train_source\",\n",
    "    path=f\"{os.environ['ROOT_DIR']}/data/train.parquet\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "validate_source = FileSource(\n",
    "    name=\"validate_source\",\n",
    "    path=f\"{os.environ['ROOT_DIR']}/data/validate.parquet\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "test_source = FileSource(\n",
    "    name=\"test_source\",\n",
    "    path=f\"{os.environ['ROOT_DIR']}/data/test.parquet\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "store.registry.apply_data_source(train_source, store.project)\n",
    "store.registry.apply_data_source(validate_source, store.project)\n",
    "store.registry.apply_data_source(test_source, store.project)\n",
    "!feast -c $FEAST_REPO data-sources list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      DESCRIPTION    TYPE\n",
      "customer                 ValueType.UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "# Customer entity\n",
    "customer = Entity(name=\"customer\", join_keys=[\"customer_id\"])\n",
    "store.registry.apply_entity(customer, store.project)\n",
    "!feast -c $FEAST_REPO entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        ENTITIES      TYPE\n",
      "training    {'customer'}  FeatureView\n",
      "validation  {'customer'}  FeatureView\n",
      "test        {'customer'}  FeatureView\n"
     ]
    }
   ],
   "source": [
    "training_fv = FeatureView(\n",
    "    name=\"training\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=train_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "validation_fv = FeatureView(\n",
    "    name=\"validation\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=validate_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "test_fv = FeatureView(\n",
    "    name=\"test\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=test_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "store.registry.apply_feature_view(training_fv, store.project)\n",
    "store.registry.apply_feature_view(validation_fv, store.project)\n",
    "store.registry.apply_feature_view(test_fv, store.project)\n",
    "!feast -c $FEAST_REPO feature-views list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run feast serve in the background\n",
    "feast_offline_server_process = subprocess.Popen([\"feast\", \"-c\", os.environ['FEAST_REPO'], \"serve_offline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feast_offline_server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007410+    4804    4596 54 14:40 pts/1    00:00:00 /usr/bin/sh -c ps -ef | grep 'serve_offline'\n",
      "1007410+    4806    4804  0 14:40 pts/1    00:00:00 grep serve_offline\n",
      "/usr/bin/sh: line 1: kill: (4436) - No such process\n"
     ]
    }
   ],
   "source": [
    "!ps -ef | grep 'serve_offline'\n",
    "!kill -9 4436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 14:40:47,902 - Registry cache expired, so refreshing\n",
      "2024-09-25 14:40:47,904 - Registry cache expired, so refreshing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<feast.entity.Entity at 0x7fd6d547bd00>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = FeatureStore(\"feast_fraud/client\")\n",
    "store.list_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4596/2941785792.py:1: FutureWarning: The behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n",
      "  datetimes = xtrain['ts'].dt.to_pydatetime().tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimes = xtrain['ts'].dt.to_pydatetime().tolist()\n",
    "len(datetimes)\n",
    "# del xtrain\n",
    "# del xval\n",
    "# del xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimes=datetimes[:5]\n",
    "len(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch historical data\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"customer_id\": [1] * len(datetimes),\n",
    "        \"event_timestamp\": datetimes,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fv = store.registry.get_feature_view('training', store.project)\n",
    "# fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 14:58:46,193 - _list_feature_views will make breaking changes. Please use _list_batch_feature_views instead. _list_feature_views will behave like _list_all_feature_views in the future.\n",
      "2024-09-25 14:58:46,195 - Registry cache expired, so refreshing\n",
      "2024-09-25 14:58:46,199 - Registry cache expired, so refreshing\n",
      "2024-09-25 14:58:46,200 - Registry cache expired, so refreshing\n",
      "2024-09-25 14:58:46,201 - Connecting FlightClient at grpc://127.0.0.1:8815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"training:distance_from_last_transaction\",\n",
    "    ]\n",
    ").to_df()\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1956-04-15 15:40:31+00:00</td>\n",
       "      <td>0.526379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1956-04-15 16:40:31+00:00</td>\n",
       "      <td>0.687956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1956-04-15 17:40:31+00:00</td>\n",
       "      <td>8.439479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1956-04-15 18:40:31+00:00</td>\n",
       "      <td>0.527061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1956-04-15 19:40:31+00:00</td>\n",
       "      <td>0.448277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id           event_timestamp  distance_from_last_transaction\n",
       "0            1 1956-04-15 15:40:31+00:00                        0.526379\n",
       "1            1 1956-04-15 16:40:31+00:00                        0.687956\n",
       "2            1 1956-04-15 17:40:31+00:00                        8.439479\n",
       "3            1 1956-04-15 18:40:31+00:00                        0.527061\n",
       "4            1 1956-04-15 19:40:31+00:00                        0.448277"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!find . -name train.parquet\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.394745Z",
     "start_time": "2024-08-19T15:45:09.051361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data is whether it's fraudulent. All other fields are inputs to the model.\n",
    "\n",
    "feature_indexes = [\n",
    "    1,  # distance_from_last_transaction\n",
    "    2,  # ratio_to_median_purchase_price\n",
    "    4,  # used_chip\n",
    "    5,  # used_pin_number\n",
    "    6,  # online_order\n",
    "]\n",
    "\n",
    "label_indexes = [\n",
    "    7  # fraud\n",
    "]\n",
    "\n",
    "X_train =apd.read_csv('data/train.csv')\n",
    "y_train = X_train.iloc[:, label_indexes]\n",
    "X_train = X_train.iloc[:, feature_indexes]\n",
    "\n",
    "X_val = pd.read_csv('data/validate.csv')\n",
    "y_val = X_val.iloc[:, label_indexes]\n",
    "X_val = X_val.iloc[:, feature_indexes]\n",
    "\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "y_test = X_test.iloc[:, label_indexes]\n",
    "X_test = X_test.iloc[:, feature_indexes]\n",
    "\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes it a lot easier for the model to learn than random (and potentially large) values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "Path(\"artifact\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"artifact/test_data.pkl\", \"wb\") as handle:\n",
    "    pickle.dump((X_test, y_test), handle)\n",
    "with open(\"artifact/scaler.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.values.ravel())\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The model is a simple, fully-connected, deep neural network, containing three hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.489856Z",
     "start_time": "2024-08-19T15:45:09.419813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = len(feature_indexes)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training a model is often the most time-consuming part of the machine learning process.  Large models can take multiple GPUs for days.  Expect the training on CPU for this very simple model to take a minute or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.664796Z",
     "start_time": "2024-08-19T15:45:09.496686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model and get performance\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=epochs, \\\n",
    "                    validation_data=(scaler.transform(X_val.values),y_val), \\\n",
    "                    verbose = True, class_weight = class_weights)\n",
    "end = time.time()\n",
    "print(f\"Training of model is complete. Took {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.845680Z",
     "start_time": "2024-08-19T15:45:29.674230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model as ONNX for easy use of ModelMesh\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "os.makedirs(\"models/fraud/1\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/1/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might include TensorFlow messages related to GPUs. You can ignore these messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the model file was created successfully\n",
    "\n",
    "The output should include the model name, size, and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.012353Z",
     "start_time": "2024-08-19T15:45:29.856416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls -alRh ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.047040Z",
     "start_time": "2024-08-19T15:45:30.029773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pickle\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.062713Z",
     "start_time": "2024-08-19T15:45:30.058023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('artifact/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "with open('artifact/test_data.pkl', 'rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ONNX inference runtime session and predict values for all test inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.210272Z",
     "start_time": "2024-08-19T15:45:30.073900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/fraud/1/model.onnx\", providers=rt.get_available_providers())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], {input_name: scaler.transform(X_test.values).astype(np.float32)}) \n",
    "y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n",
    "threshold = 0.95\n",
    "y_pred = np.where(y_pred_temp > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.644142Z",
     "start_time": "2024-08-19T15:45:30.221686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "y_test_arr = y_test.to_numpy().squeeze()\n",
    "correct = np.equal(y_pred, y_test_arr).sum().item()\n",
    "acc = (correct / len(y_pred)) * 100\n",
    "precision = precision_score(y_test_arr, np.round(y_pred))\n",
    "recall = recall_score(y_test_arr, np.round(y_pred))\n",
    "\n",
    "print(f\"Eval Metrics: \\n Accuracy: {acc:>0.1f}%, \"\n",
    "      f\"Precision: {precision:.4f}, Recall: {recall:.4f} \\n\")\n",
    "\n",
    "c_matrix = confusion_matrix(y_test_arr, y_pred)\n",
    "ConfusionMatrixDisplay(c_matrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Is Sally's transaction likely to be fraudulent?\n",
    "\n",
    "Here is the order of the fields from Sally's transaction details:\n",
    "* distance_from_last_transaction\n",
    "* ratio_to_median_price\n",
    "* used_chip \n",
    "* used_pin_number\n",
    "* online_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.679688Z",
     "start_time": "2024-08-19T15:45:30.669086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sally_transaction_details = [\n",
    "    [0.3111400080477545,\n",
    "    1.9459399775518593, \n",
    "    1.0, \n",
    "    0.0, \n",
    "    0.0]\n",
    "    ]\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(sally_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Is Sally's transaction predicted to be fraudulent? (true = YES, false = NO) \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely was Sally's transaction to be fraudulent? \")\n",
    "print(\"{:.5f}\".format(np.squeeze(prediction)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.722273Z",
     "start_time": "2024-08-19T15:45:30.719926Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.756156Z",
     "start_time": "2024-08-19T15:45:30.750131Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "63462a1f26ab486248b2a0fd058a0d9f9a6566a80083a3e1eb8f35617f2381b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
