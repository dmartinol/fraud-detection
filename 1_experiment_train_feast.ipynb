{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:05.830869Z",
     "start_time": "2024-08-19T15:45:04.819700Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mypy-protobuf 3.6.0 requires protobuf>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "feast 0.40.1 requires protobuf<5.0.0,>=4.24.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "codeflare-sdk 0.19.1 requires pydantic<2, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q onnx onnxruntime tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies for the model training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:08.983925Z",
     "start_time": "2024-08-19T15:45:05.835311Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 07:31:01.311457: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 07:31:01.311524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 07:31:01.312922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 07:31:01.320266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 07:31:02.204275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tf2onnx\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might show TensorFlow messages, such as a \"Could not find TensorRT\" warning. You can ignore these messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV data\n",
    "\n",
    "The CSV data that you use to train the model contains the following fields:\n",
    "\n",
    "* **distancefromhome** - The distance from home where the transaction happened.\n",
    "* **distancefromlast_transaction** - The distance from the last transaction that happened.\n",
    "* **ratiotomedianpurchaseprice** - The ratio of purchased price compared to median purchase price.\n",
    "* **repeat_retailer** - If it's from a retailer that already has been purchased from before.\n",
    "* **used_chip** - If the credit card chip was used.\n",
    "* **usedpinnumber** - If the PIN number was used.\n",
    "* **online_order** - If it was an online order.\n",
    "* **fraud** - If the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Feast project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psycopg                      3.2.2\n",
      "psycopg-pool                 3.2.3\n",
      "Feast SDK Version: \"0.40.1\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q feast\n",
    "!pip install -q psycopg==3.2.2\n",
    "!pip install -q psycopg-pool==3.2.3\n",
    "!pip list | grep psyco\n",
    "!feast version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forward Feast logs to the notebook output\n",
    "import logging\n",
    "import sys\n",
    "from io import StringIO\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FEAST_REPO=feast_fraud/feature_repo\n",
      "env: ROOT_DIR=/opt/app-root/src/fraud-detection\n",
      "data  feature_store.yaml\n"
     ]
    }
   ],
   "source": [
    "%env FEAST_REPO=feast_fraud/feature_repo\n",
    "%env ROOT_DIR=/opt/app-root/src/fraud-detection\n",
    "!ls $FEAST_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mNo changes to registry\n",
      "\u001b[1m\u001b[94mNo changes to infrastructure\n",
      "NAME    DESCRIPTION    TYPE\n",
      "NAME    CLASS\n",
      "NAME    ENTITIES    TYPE\n"
     ]
    }
   ],
   "source": [
    "!feast -c $FEAST_REPO apply\n",
    "!feast -c $FEAST_REPO entities list\n",
    "!feast -c $FEAST_REPO data-sources list\n",
    "!feast -c $FEAST_REPO feature-views list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from feast import (\n",
    "    # Entity,\n",
    "    FeatureView,\n",
    "    Field,\n",
    ")\n",
    "from feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source import (\n",
    "    PostgreSQLSource,\n",
    ")\n",
    "from feast.feature_store import FeatureStore\n",
    "from feast.feature_logging import LoggingConfig\n",
    "from feast.infra.offline_stores.file_source import FileLoggingDestination\n",
    "from feast.on_demand_feature_view import on_demand_feature_view\n",
    "from feast.types import Float32, Float64, Int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "~~1. Add entity key to the tables~~\n",
    "1. Add event_ts field\n",
    "1. Convert to parquet files\n",
    "\n",
    "~~Since the original dataset was not considering the `customer` concept, we'll replicate the same setup: add a new field customer_id and apply the same value to each record.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def add_customer_id(df):\n",
    "#     df['customer_id'] = 1\n",
    "\n",
    "def add_timestamps(df):\n",
    "    # Create time series: one entry every 1H, up to now\n",
    "    timestamps = pd.date_range(\n",
    "        end=pd.Timestamp.now().replace(microsecond=0), \n",
    "        periods=len(df), \n",
    "        freq='1H').to_frame(name=\"ts\", index=False)\n",
    "\n",
    "    timestamps['created'] = timestamps['ts']\n",
    "    df = pd.concat(objs=[df, timestamps], axis=1)\n",
    "    columns = df.columns.tolist()\n",
    "    columns.insert(0, columns.pop(9))\n",
    "    columns.insert(0, columns.pop(9))\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2927/3986762096.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n",
      "/tmp/ipykernel_2927/3986762096.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n",
      "/tmp/ipykernel_2927/3986762096.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----xtrain-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   ts                              600000 non-null  datetime64[ns]\n",
      " 1   created                         600000 non-null  datetime64[ns]\n",
      " 2   distance_from_home              600000 non-null  float64       \n",
      " 3   distance_from_last_transaction  600000 non-null  float64       \n",
      " 4   ratio_to_median_purchase_price  600000 non-null  float64       \n",
      " 5   repeat_retailer                 600000 non-null  float64       \n",
      " 6   used_chip                       600000 non-null  float64       \n",
      " 7   used_pin_number                 600000 non-null  float64       \n",
      " 8   online_order                    600000 non-null  float64       \n",
      " 9   fraud                           600000 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(8)\n",
      "memory usage: 45.8 MB\n",
      "-----len(xtrain)-----\n",
      "600000\n",
      "-----len(xval)-----\n",
      "200000\n",
      "-----len(xtest)-----\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "xtrain = pd.read_csv('data/train.csv')\n",
    "xval = pd.read_csv('data/validate.csv')\n",
    "xtest = pd.read_csv('data/test.csv')\n",
    "\n",
    "# add_customer_id(xtrain)\n",
    "# add_customer_id(xval)\n",
    "# add_customer_id(xtest)\n",
    "\n",
    "xtrain = add_timestamps(xtrain)\n",
    "xval = add_timestamps(xval)\n",
    "xtest = add_timestamps(xtest)\n",
    "\n",
    "!rm data/*.parquet\n",
    "xtrain.to_parquet('data/train.parquet')\n",
    "xval.to_parquet('data/validate.parquet')\n",
    "xtest.to_parquet('data/test.parquet')\n",
    "\n",
    "print(\"-----xtrain-----\")\n",
    "xtrain.info()\n",
    "print(\"-----len(xtrain)-----\")\n",
    "print(len(xtrain))\n",
    "print(\"-----len(xval)-----\")\n",
    "print(len(xval))\n",
    "print(\"-----len(xtest)-----\")\n",
    "print(len(xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 1001210000 1001210000 8.8M Sep 26 07:36 data/test.parquet\n",
      "-rw-r--r--. 1 1001210000 1001210000  24M Sep 26 07:36 data/train.parquet\n",
      "-rw-r--r--. 1 1001210000 1001210000 8.8M Sep 26 07:36 data/validate.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/*parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a SQL store\n",
    "A Postgres service is deployed on the current namespace and DB tables are created and populated with data from the `xtrain`, `xval` and `xtest` data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current namespace is dmartino-fraud-detection\n",
      "dmartino-fraud-detection\n"
     ]
    }
   ],
   "source": [
    "namespace_path='/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "with open(namespace_path, \"r\") as file:\n",
    "    current_namespace = file.read().strip()\n",
    "print(f\"Current namespace is {current_namespace}\")\n",
    "os.environ['CURRENT_NS'] = current_namespace\n",
    "!echo $CURRENT_NS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy PostgreSQL from template\n",
    "From the OpenShift console, create an instance of PostgreSQL database with the following options in the current namespace:\n",
    "* DATABASE_SERVICE_NAME=postgresql \n",
    "* POSTGRESQL_USER=feast \n",
    "* POSTGRESQL_PASSWORD=feast\n",
    "* POSTGRESQL_DATABASE=feast \n",
    "* VOLUME_CAPACITY=2Gi \n",
    "* MEMORY_LIMIT=1Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup DB connection attributes\n",
    "psqlHost = f'postgresql.{current_namespace}.svc.cluster.local'\n",
    "psqlPort = 5432\n",
    "psqlUsername = 'feast'\n",
    "psqlPassword = 'feast'\n",
    "psqlDb = 'feast'\n",
    "psqlSchema = 'public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting from fraud_train\n",
      "Rows before deletion: 0\n",
      "Deleting from fraud_validate\n",
      "Rows before deletion: 0\n",
      "Deleting from fraud_test\n",
      "Rows before deletion: 0\n",
      "Persisting xtrain...\n",
      "Persisting xval...\n",
      "Persisting xtest...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load DataFrame to DB using `to_sql` method of pandas DataFrame\n",
    "import psycopg\n",
    "from sqlalchemy import create_engine, text, Table, MetaData, select, func\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg://{psqlUsername}:{psqlPassword}@{psqlHost}:{str(psqlPort)}/{psqlDb}')\n",
    "\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "for t in ['fraud_train', 'fraud_validate', 'fraud_test']:\n",
    "    if t in metadata.tables:\n",
    "        table = metadata.tables[t]\n",
    "        with engine.connect() as connection:\n",
    "            row_count = connection.execute(select(func.count()).select_from(text(t))).scalar()\n",
    "            print(f\"Deleting from {t}\")\n",
    "            print(f\"Rows before deletion: {row_count}\")\n",
    "            connection.execute(table.delete())\n",
    "            connection.commit()\n",
    "\n",
    "print(\"Persisting xtrain...\")\n",
    "xtrain.to_sql('fraud_train', engine, if_exists='append', index=True, schema=psqlSchema)\n",
    "print(\"Persisting xval...\")\n",
    "xval.to_sql('fraud_validate', engine, if_exists='append', index=True, schema=psqlSchema)\n",
    "print(\"Persisting xtest...\")\n",
    "xtest.to_sql('fraud_test', engine, if_exists='append', index=True, schema=psqlSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in fraud_train: 600000\n",
      "Rows in fraud_validate: 200000\n",
      "Rows in fraud_test: 200000\n"
     ]
    }
   ],
   "source": [
    "# Validate row count\n",
    "for t in ['fraud_train', 'fraud_validate', 'fraud_test']:\n",
    "    if t in metadata.tables:\n",
    "        table = metadata.tables[t]\n",
    "        with engine.connect() as connection:\n",
    "            row_count = connection.execute(select(func.count()).select_from(text(t))).scalar()\n",
    "            print(f\"Rows in {t}: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Feature Store\n",
    "* Map parquet files to `PostgreSQLSource`s\n",
    "* Define FeatureViews for training purposes\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we cannot apply feature store definitions from the remote servers because of GH issue [4592: Remote apply](https://github.com/feast-dev/feast/issues/4529), so we use a direct connection to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: feast_fraud\n",
      "registry:\n",
      "    registry_type: sql\n",
      "    path: postgresql+psycopg://feast:feast@postgresql:5432/feast\n",
      "    cache_ttl_seconds: 60\n",
      "    sqlalchemy_config_kwargs:\n",
      "        echo: false\n",
      "        pool_pre_ping: true\n",
      "online_store:\n",
      "    type: postgres\n",
      "    host: postgresql\n",
      "    port: 5432\n",
      "    database: feast\n",
      "    db_schema: public\n",
      "    user: feast\n",
      "    password: feast\n",
      "offline_store:\n",
      "    type: postgres\n",
      "    host: postgresql\n",
      "    port: 5432\n",
      "    database: feast\n",
      "    db_schema: public\n",
      "    user: feast\n",
      "    password: feast\n",
      "entity_key_serialization_version: 2\n"
     ]
    }
   ],
   "source": [
    "!cat $FEAST_REPO/feature_store.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the store\n",
    "store = FeatureStore(os.environ['FEAST_REPO'])\n",
    "print(store.list_entities())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             CLASS\n",
      "train_source     <class 'feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source.PostgreSQLSource'>\n",
      "validate_source  <class 'feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source.PostgreSQLSource'>\n",
      "test_source      <class 'feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source.PostgreSQLSource'>\n",
      "CPU times: user 38 ms, sys: 27.9 ms, total: 65.9 ms\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the PostgreSQLSource\n",
    "train_source = PostgreSQLSource(\n",
    "    name=\"train_source\",\n",
    "    query=\"SELECT * FROM fraud_train\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "validate_source = PostgreSQLSource(\n",
    "    name=\"validate_source\",\n",
    "    query=\"SELECT * FROM fraud_validate\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "test_source = PostgreSQLSource(\n",
    "    name=\"test_source\",\n",
    "    query=\"SELECT * FROM fraud_test\",\n",
    "    timestamp_field=\"ts\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "store.registry.apply_data_source(train_source, store.project)\n",
    "store.registry.apply_data_source(validate_source, store.project)\n",
    "store.registry.apply_data_source(test_source, store.project)\n",
    "!feast -c $FEAST_REPO data-sources list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    DESCRIPTION    TYPE\n"
     ]
    }
   ],
   "source": [
    "# Customer entity\n",
    "# customer = Entity(name=\"customer\", join_keys=[\"customer_id\"])\n",
    "# store.registry.apply_entity(customer, store.project)\n",
    "!feast -c $FEAST_REPO entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/feast/feature_view.py:425: UserWarning: There are some mismatches in your feature view's registered entities. Please check if you have applied your entities correctly.Entities: ['__dummy'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "NAME           ENTITIES    TYPE\n",
      "training_fv    n/a         FeatureView\n",
      "validation_fv  n/a         FeatureView\n",
      "test_fv        n/a         FeatureView\n",
      "CPU times: user 29.5 ms, sys: 37 ms, total: 66.5 ms\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_fv = FeatureView(\n",
    "    name=\"training_fv\",\n",
    "    # entities=[customer],\n",
    "    entities=[],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        # Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=train_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "validation_fv = FeatureView(\n",
    "    name=\"validation_fv\",\n",
    "    # entities=[customer],\n",
    "    entities=[],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        # Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=validate_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "test_fv = FeatureView(\n",
    "    name=\"test_fv\",\n",
    "    # entities=[customer],\n",
    "    entities=[],\n",
    "    ttl=timedelta(days=1),\n",
    "    schema=[\n",
    "        # Field(name=\"customer_id\", dtype=Int64),\n",
    "        Field(name=\"distance_from_last_transaction\", dtype=Float64),\n",
    "        Field(name=\"ratio_to_median_purchase_price\", dtype=Float64),\n",
    "        Field(name=\"used_chip\", dtype=Float64),\n",
    "        Field(name=\"used_pin_number\", dtype=Float64),\n",
    "        Field(name=\"online_order\", dtype=Float64),\n",
    "        Field(name=\"fraud\", dtype=Float64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=test_source,\n",
    "    tags={\"team\": \"training\"},\n",
    ")\n",
    "store.registry.apply_feature_view(training_fv, store.project)\n",
    "store.registry.apply_feature_view(validation_fv, store.project)\n",
    "store.registry.apply_feature_view(test_fv, store.project)\n",
    "!feast -c $FEAST_REPO feature-views list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Feast services\n",
    "A fully distributed Feast environment is deployed using Helm:\n",
    "* Registry\n",
    "* Online Store\n",
    "* Offline Store\n",
    "\n",
    "Run the following commands from a local clone of this git repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate base64 encoded feature configurations\n",
    "```console\n",
    "REGISTRY_CONFIG_BASE64=$(cat feast_fraud/feature_repo/registry_store.yaml | base64 -w0)\n",
    "ONLINE_CONFIG_BASE64=$(cat feast_fraud/feature_repo/online_store.yaml | base64 -w0)\n",
    "OFFLINE_CONFIG_BASE64=$(cat feast_fraud/feature_repo/offline_store.yaml | base64 -w0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Initialize the image settings:\n",
    "```console\n",
    "FEAST_IMAGE_REPO=feastdev/feature-server\n",
    "FEAST_IMAGE_VERSION=latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Helm repository:\n",
    "```console\n",
    "helm repo add feast-charts https://feast-helm-charts.storage.googleapis.com\n",
    "helm repo update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to the cluster and set the current project as the default.\n",
    "\n",
    "Then run the following command to install the Registry server:\n",
    "```console\n",
    "helm upgrade --install feast-registry feast-charts/feast-feature-server \\\n",
    "--set fullnameOverride=registry-server --set feast_mode=registry \\\n",
    "--set image.repository=${FEAST_IMAGE_REPO} --set image.tag=${FEAST_IMAGE_VERSION} \\\n",
    "--set feature_store_yaml_base64=$REGISTRY_CONFIG_BASE64\n",
    "\n",
    "oc wait --for=condition=available deployment/registry-server --timeout=2m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to install the Offline server:\n",
    "```console\n",
    "helm upgrade --install feast-offline feast-charts/feast-feature-server \\\n",
    "--set fullnameOverride=offline-server --set feast_mode=offline \\\n",
    "--set image.repository=${FEAST_IMAGE_REPO} --set image.tag=${FEAST_IMAGE_VERSION} \\\n",
    "--set feature_store_yaml_base64=$OFFLINE_CONFIG_BASE64\n",
    "\n",
    "oc wait --for=condition=available deployment/offline-server --timeout=2m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to install the Online server:\n",
    "```console\n",
    "helm upgrade --install feast-online feast-charts/feast-feature-server \\\n",
    "--set fullnameOverride=online-server --set feast_mode=online \\\n",
    "--set image.repository=${FEAST_IMAGE_REPO} --set image.tag=${FEAST_IMAGE_VERSION} \\\n",
    "--set feature_store_yaml_base64=$ONLINE_CONFIG_BASE64\n",
    "\n",
    "oc wait --for=condition=available deployment/online-server --timeout=2m\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # Run feast serve in the background\n",
    "# feast_offline_server_process = subprocess.Popen([\"feast\", \"-c\", os.environ['FEAST_REPO'], \"serve_offline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feast_offline_server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ps -ef | grep 'serve_offline'\n",
    "!kill -9 788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store = FeatureStore(\"feast_fraud/client\")\n",
    "store.list_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetimes = xtrain['ts'].dt.to_pydatetime().tolist()\n",
    "datetimes=datetimes[:10]\n",
    "len(datetimes)\n",
    "# del xtrain\n",
    "# del xval\n",
    "# del xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datetimes=datetimes[:5]\n",
    "# len(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch historical data\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        # \"customer_id\": [1] * len(datetimes),\n",
    "        \"event_timestamp\": datetimes,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fv = store.registry.get_feature_view('training', store.project)\n",
    "# fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"training:distance_from_last_transaction\",\n",
    "        \"training:ratio_to_median_purchase_price\",\n",
    "        \"training:used_chip\",\n",
    "        \"training:used_pin_number\",\n",
    "        \"training:online_order\",\n",
    "        \"training:fraud\",\n",
    "    ]\n",
    ").to_df()\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!find . -name train.parquet\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.394745Z",
     "start_time": "2024-08-19T15:45:09.051361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data is whether it's fraudulent. All other fields are inputs to the model.\n",
    "\n",
    "feature_indexes = [\n",
    "    1,  # distance_from_last_transaction\n",
    "    2,  # ratio_to_median_purchase_price\n",
    "    4,  # used_chip\n",
    "    5,  # used_pin_number\n",
    "    6,  # online_order\n",
    "]\n",
    "\n",
    "label_indexes = [\n",
    "    7  # fraud\n",
    "]\n",
    "\n",
    "X_train =apd.read_csv('data/train.csv')\n",
    "y_train = X_train.iloc[:, label_indexes]\n",
    "X_train = X_train.iloc[:, feature_indexes]\n",
    "\n",
    "X_val = pd.read_csv('data/validate.csv')\n",
    "y_val = X_val.iloc[:, label_indexes]\n",
    "X_val = X_val.iloc[:, feature_indexes]\n",
    "\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "y_test = X_test.iloc[:, label_indexes]\n",
    "X_test = X_test.iloc[:, feature_indexes]\n",
    "\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes it a lot easier for the model to learn than random (and potentially large) values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "Path(\"artifact\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"artifact/test_data.pkl\", \"wb\") as handle:\n",
    "    pickle.dump((X_test, y_test), handle)\n",
    "with open(\"artifact/scaler.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.values.ravel())\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The model is a simple, fully-connected, deep neural network, containing three hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.489856Z",
     "start_time": "2024-08-19T15:45:09.419813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = len(feature_indexes)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training a model is often the most time-consuming part of the machine learning process.  Large models can take multiple GPUs for days.  Expect the training on CPU for this very simple model to take a minute or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.664796Z",
     "start_time": "2024-08-19T15:45:09.496686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model and get performance\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=epochs, \\\n",
    "                    validation_data=(scaler.transform(X_val.values),y_val), \\\n",
    "                    verbose = True, class_weight = class_weights)\n",
    "end = time.time()\n",
    "print(f\"Training of model is complete. Took {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.845680Z",
     "start_time": "2024-08-19T15:45:29.674230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model as ONNX for easy use of ModelMesh\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "os.makedirs(\"models/fraud/1\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/1/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might include TensorFlow messages related to GPUs. You can ignore these messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the model file was created successfully\n",
    "\n",
    "The output should include the model name, size, and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.012353Z",
     "start_time": "2024-08-19T15:45:29.856416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls -alRh ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.047040Z",
     "start_time": "2024-08-19T15:45:30.029773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pickle\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.062713Z",
     "start_time": "2024-08-19T15:45:30.058023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('artifact/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "with open('artifact/test_data.pkl', 'rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ONNX inference runtime session and predict values for all test inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.210272Z",
     "start_time": "2024-08-19T15:45:30.073900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/fraud/1/model.onnx\", providers=rt.get_available_providers())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], {input_name: scaler.transform(X_test.values).astype(np.float32)}) \n",
    "y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n",
    "threshold = 0.95\n",
    "y_pred = np.where(y_pred_temp > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.644142Z",
     "start_time": "2024-08-19T15:45:30.221686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "y_test_arr = y_test.to_numpy().squeeze()\n",
    "correct = np.equal(y_pred, y_test_arr).sum().item()\n",
    "acc = (correct / len(y_pred)) * 100\n",
    "precision = precision_score(y_test_arr, np.round(y_pred))\n",
    "recall = recall_score(y_test_arr, np.round(y_pred))\n",
    "\n",
    "print(f\"Eval Metrics: \\n Accuracy: {acc:>0.1f}%, \"\n",
    "      f\"Precision: {precision:.4f}, Recall: {recall:.4f} \\n\")\n",
    "\n",
    "c_matrix = confusion_matrix(y_test_arr, y_pred)\n",
    "ConfusionMatrixDisplay(c_matrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Is Sally's transaction likely to be fraudulent?\n",
    "\n",
    "Here is the order of the fields from Sally's transaction details:\n",
    "* distance_from_last_transaction\n",
    "* ratio_to_median_price\n",
    "* used_chip \n",
    "* used_pin_number\n",
    "* online_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.679688Z",
     "start_time": "2024-08-19T15:45:30.669086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sally_transaction_details = [\n",
    "    [0.3111400080477545,\n",
    "    1.9459399775518593, \n",
    "    1.0, \n",
    "    0.0, \n",
    "    0.0]\n",
    "    ]\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(sally_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Is Sally's transaction predicted to be fraudulent? (true = YES, false = NO) \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely was Sally's transaction to be fraudulent? \")\n",
    "print(\"{:.5f}\".format(np.squeeze(prediction)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.722273Z",
     "start_time": "2024-08-19T15:45:30.719926Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.756156Z",
     "start_time": "2024-08-19T15:45:30.750131Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "63462a1f26ab486248b2a0fd058a0d9f9a6566a80083a3e1eb8f35617f2381b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
